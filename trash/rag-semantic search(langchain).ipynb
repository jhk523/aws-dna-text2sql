{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q boto3\n",
    "!pip install -q requests\n",
    "!pip install -q requests-aws4auth\n",
    "!pip install -q opensearch-py\n",
    "!pip install -q tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install \"faiss-cpu\" --quiet\n",
    "!pip install langchain --quiet\n",
    "!pip install jq --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from langchain.document_loaders.json_loader import JSONLoader\n",
    "from langchain.docstore.document import Document\n",
    "import json\n",
    "import re\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from functools import reduce\n",
    "from langchain.prompts import PromptTemplate\n",
    "from sqlalchemy import MetaData\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import sqlite3\n",
    "\n",
    "data_path = './data'\n",
    "with open(f'{data_path}/tables.json', 'rb') as ofp:\n",
    "    meta = json.load(ofp)\n",
    "data = meta[0]\n",
    "\n",
    "data = [i for i in meta if i['db_id'] == 'department_store']\n",
    "\n",
    "data  = data[0]    \n",
    "columns = data[\"column_names_original\"]\n",
    "col_df = pd.DataFrame(columns).iloc[1:]\n",
    "col_df.rename(columns={0: 'table_idx', 1: 'col_name'}, inplace=True)\n",
    "col_df\n",
    "\n",
    "types_df = pd.DataFrame(data[\"column_types\"]).iloc[1:]\n",
    "types_df.rename(columns={0: 'type'}, inplace=True)\n",
    "types_df\n",
    "\n",
    "merged_col = pd.concat([col_df, types_df], axis=1)\n",
    "\n",
    "tables_df = pd.DataFrame(data[\"table_names_original\"])\n",
    "tables_df.reset_index(inplace=True)\n",
    "tables_df.columns = ['table_idx', 'table_name']\n",
    "\n",
    "meta = pd.merge(tables_df, merged_col, on=['table_idx'])\n",
    "meta = meta.drop(columns=['table_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "DB_NAME = \"text2sql\"\n",
    "DB_FAISS_PATH = './vectorstore/db_faiss'\n",
    "\n",
    "bedrock_region = athena_region = boto3.session.Session().region_name\n",
    "retry_config = Config(retries = {'max_attempts': 100})\n",
    "session = boto3.Session(region_name=bedrock_region)\n",
    "bedrock = session.client('bedrock-runtime', region_name=bedrock_region, config=retry_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def ask_llm(question):\n",
    "    bedrock_model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "    body = json.dumps({\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                \"max_tokens\": 1024,\n",
    "                \"temperature\" : 0.1,\n",
    "                \"top_p\": 0.5,\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": question},\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "            }) \n",
    "\n",
    "    response = bedrock.invoke_model(\n",
    "        body=body, \n",
    "        modelId=bedrock_model_id,\n",
    "        accept='application/json',\n",
    "        contentType='application/json') #payload를 Bedrock으로 전송\n",
    "\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    llm_output = response_body.get(\"content\")[0].get(\"text\")\n",
    "    return llm_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create new docs with the right metadata we need for indexing\n",
    "def create_docs_with_correct_metadata(documents):\n",
    "    # We are going to return a list of new documents\n",
    "    new_docs = []\n",
    "\n",
    "    # For each document\n",
    "    for doc in documents:\n",
    "        # Get it's metadata and contents\n",
    "        metadata = doc.metadata\n",
    "        contents = json.loads(doc.page_content)\n",
    "\n",
    "        # Now calculate the new metadata that we want to add\n",
    "        new_metadata = {\n",
    "            \"tableName\": contents[\"tableName\"],\n",
    "            \"question\": contents[\"question\"],\n",
    "            \"tableSchema\": contents[\"tableSchema\"],\n",
    "        }\n",
    "\n",
    "        # Print out the new metadata for our documents\n",
    "        # print(new_metadata)\n",
    "\n",
    "        new_docs.append(\n",
    "            Document(page_content=new_metadata[\"question\"], metadata=new_metadata)\n",
    "        )\n",
    "\n",
    "    return new_docs\n",
    "\n",
    "def load_json_file(filename):\n",
    "    loader = JSONLoader(file_path=filename, jq_schema=\".[]\", text_content=False)\n",
    "\n",
    "    # This is our internal Langchain document data structure\n",
    "    docs = loader.load()\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function asks the LLM to inspect a table schema, generate some questions which could be answered\n",
    "# by that schema, and then it stores those questions to file, loads them all into a single vectorDB\n",
    "def add_new_table(schema, table_name, is_incremental, bedrock_embeddings):\n",
    "    \"\"\"\n",
    "    :schema         :   \n",
    "    :table_name     :\n",
    "    :model_id       :\n",
    "    :is_incremental :\n",
    "    \"\"\"\n",
    "    print(f\"Adding table {table_name} with schema {schema}\")\n",
    "    \n",
    "    question = f\"\"\"\n",
    "    \\n\\nHuman: \n",
    "    only return the a bulleted numbered list of unique and detailed questions that could be answered by this table called {table_name} with schema:\n",
    "    {schema}.\n",
    "    Instructions:\n",
    "        Use natural language descriptions only.\n",
    "        Do not use SQL.\n",
    "        Produced a varied list of questions, but the questions should be unique and detailed.\n",
    "        The questions should be in a format that is easy to understand and answer.\n",
    "        Ask about as much of the information in the table as possible.\n",
    "        You can ask about more than one aspect of the data at a time.\n",
    "        Qustions should begin with, 'What', 'Which', 'How', 'When' or 'Can'. Use variable names. \n",
    "        The questions should use relevant buisness vocabularly and terminology only. \n",
    "        Do not use column names in your output - use relevant natural language descriptions only. \n",
    "        Do not output any numeric values.\n",
    "        Output questions starting with bulleted numbered list. \n",
    "         \n",
    "        \\n Questions: 1.\n",
    "        \\n Assistant:\n",
    "        \"\"\"\n",
    "       \n",
    "    answer = ask_llm(question)\n",
    "    os.makedirs('./data/rag', exist_ok=True)\n",
    "    question_list_filename = f\"./data/rag/questionList{table_name}.json\"\n",
    "\n",
    "    # # Get rid of anything before the 1.\n",
    "    # if re.match(r\"^[^\\d+]\\. \", answer) and re.search(r\"\\d+\\. \", answer):\n",
    "    #     answer = \"1. \" + answer.split(\"1. \")[1]\n",
    "    # else:\n",
    "    #     answer = \"1. \" + answer\n",
    "\n",
    "    print(\n",
    "        f\"Writing questions to {question_list_filename}, with schema {schema}, with table name {table_name} and answer {answer}.\\n\\n\"\n",
    "    )\n",
    "\n",
    "    write_questions_to_file(question_list_filename, table_name, schema, answer)\n",
    "\n",
    "    docs = load_json_file(question_list_filename)\n",
    "    docs = create_docs_with_correct_metadata(docs)\n",
    "    new_questions = FAISS.from_documents(docs, bedrock_embeddings)\n",
    "    db_exists = True if os.path.exists(f\"{DB_FAISS_PATH}/index.faiss\") else False\n",
    "    # Add new tables\n",
    "    if is_incremental and db_exists:\n",
    "            question_db = FAISS.load_local(DB_FAISS_PATH, bedrock_embeddings, allow_dangerous_deserialization=True)\n",
    "            question_db.merge_from(new_questions)\n",
    "            question_db.save_local(DB_FAISS_PATH)\n",
    "\n",
    "    # Load for the first time\n",
    "    else:\n",
    "        print(f\"is_incremental set to {str(is_incremental)} and/or no vector db found. Creating...\")\n",
    "        new_questions.save_local(DB_FAISS_PATH)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_questions_to_file(question_list_filename, table_name, table_schema, answer):\n",
    "    data_list = []\n",
    "    question_list_obj = answer\n",
    "    questions_list = question_list_obj.splitlines()\n",
    "    # Open the file in write mode\n",
    "    with open(question_list_filename, mode=\"w\", newline=\"\") as file:\n",
    "        for question in questions_list:\n",
    "\n",
    "            # Skip if it doesn't really have a question\n",
    "            if \"?\" not in question:\n",
    "                continue\n",
    "\n",
    "            questionSplit = re.split(r\"\\d{1,5}.||. ||- \", question, maxsplit=1)\n",
    "            question = questionSplit[1]\n",
    "            data = {\n",
    "                \"tableName\": table_name,\n",
    "                \"question\": question,\n",
    "                \"tableSchema\": table_schema.lstrip(\" \"),\n",
    "            }\n",
    "            data_list.append(data)\n",
    "\n",
    "        json.dump(data_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('address_id|address_details', 'Addresses')\n",
      "('customer_id|address_id|date_from|date_to', 'Customer_Addresses')\n",
      "('order_id|customer_id|order_status_code|order_date', 'Customer_Orders')\n",
      "('customer_id|payment_method_code|customer_code|customer_name|customer_address|customer_phone|customer_email', 'Customers')\n",
      "('dept_store_chain_id|dept_store_chain_name', 'Department_Store_Chain')\n",
      "('dept_store_id|dept_store_chain_id|store_name|store_address|store_phone|store_email', 'Department_Stores')\n",
      "('department_id|dept_store_id|department_name', 'Departments')\n",
      "('order_item_id|order_id|product_id', 'Order_Items')\n",
      "('product_id|supplier_id|date_supplied_from|date_supplied_to|total_amount_purchased|total_value_purchased', 'Product_Suppliers')\n",
      "('product_id|product_type_code|product_name|product_price', 'Products')\n",
      "('staff_id|staff_gender|staff_name', 'Staff')\n",
      "('staff_id|department_id|date_assigned_from|job_title_code|date_assigned_to', 'Staff_Department_Assignments')\n",
      "('supplier_id|address_id|date_from|date_to', 'Supplier_Addresses')\n",
      "('supplier_id|supplier_name|supplier_phone', 'Suppliers')\n"
     ]
    }
   ],
   "source": [
    "new_meta = meta.groupby(['table_name'])['col_name'].apply(list).reset_index()\n",
    "new_meta = new_meta.set_index('table_name')\n",
    "\n",
    "tpc_ds = []\n",
    "for idx, row in new_meta.iterrows():\n",
    "    v = (('|').join(row.values[0]), idx)\n",
    "    print(v)\n",
    "    tpc_ds.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BedrockEmbeddings(client=<botocore.client.BedrockRuntime object at 0x7f4a0ddf6f20>, region_name=None, credentials_profile_name=None, model_id='amazon.titan-embed-text-v1', model_kwargs=None, endpoint_url=None, normalize=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bedrock_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('address_id|address_details', 'Addresses')\n",
      "Adding table Addresses with schema address_id|address_details\n",
      "Writing questions to ./data/rag/questionListAddresses.json, with schema address_id|address_details, with table name Addresses and answer • 1. What is the total number of unique addresses in the system?\n",
      "\n",
      "• 2. Which addresses have specific details or characteristics (e.g., residential, commercial, located in a particular city or region)?\n",
      "\n",
      "• 3. How many addresses are associated with a particular customer or entity?\n",
      "\n",
      "• 4. Can you provide a list of addresses that meet certain criteria (e.g., within a specific zip code range, containing a specific keyword in the address details)?\n",
      "\n",
      "• 5. What is the distribution of addresses across different geographic regions or areas?\n",
      "\n",
      "• 6. Which addresses have been recently added or updated in the system?\n",
      "\n",
      "• 7. How can I identify duplicate or potentially redundant address entries?\n",
      "\n",
      "• 8. Can you provide a summary of address details for a specific subset of addresses (e.g., addresses associated with a particular customer segment or business unit)?\n",
      "\n",
      "• 9. What is the typical format or structure of the address details field, and how can I extract specific components (e.g., street name, city, state) from it?\n",
      "\n",
      "• 10. Which addresses have incomplete or missing details, and how can I identify them for further investigation or data enrichment?.\n",
      "\n",
      "\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from functools import reduce\n",
    "\n",
    "# model_name = 'Titan-Embeddings-G1'\n",
    "bedrock_embeddings = BedrockEmbeddings(client=bedrock)\n",
    "for x in tpc_ds:\n",
    "    print(x)\n",
    "    add_new_table(\n",
    "        schema=x[0], \n",
    "        table_name=x[1],\n",
    "        is_incremental=True, \n",
    "        bedrock_embeddings=bedrock_embeddings\n",
    "    )\n",
    "    print('-------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'• 1. What is the total number of unique addresses in the system?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='• 1. What is the total number of unique addresses in the system?', metadata={'tableName': 'Addresses', 'question': '• 1. What is the total number of unique addresses in the system?', 'tableSchema': 'address_id|address_details'})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tableName': 'Addresses',\n",
       " 'question': '• 1. What is the total number of unique addresses in the system?',\n",
       " 'tableSchema': 'address_id|address_details'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseModel.schema of <class 'langchain_core.documents.base.Document'>>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question_db = FAISS.load_local(DB_FAISS_PATH, bedrock_embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Department_Stores': 'dept_store_id|dept_store_chain_id|store_name|store_address|store_phone|store_email',\n",
       " 'Customers': 'customer_id|payment_method_code|customer_code|customer_name|customer_address|customer_phone|customer_email',\n",
       " 'Supplier_Addresses': 'supplier_id|address_id|date_from|date_to'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"aws 전화번호 알려줘\"\n",
    "schema =  {}\n",
    "\n",
    "results_with_scores = question_db.similarity_search_with_score(query, k=10)\n",
    "for doc, score in results_with_scores:\n",
    "    # print(doc.metadata['question'])\n",
    "    schema[doc.metadata['tableName']] = doc.metadata['tableSchema']\n",
    "display(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list_filename = f\"./data/rag/questionListAddresses.json\"\n",
    "\n",
    "# # Get rid of anything before the 1.\n",
    "# if re.match(r\"^[^\\d+]\\. \", answer) and re.search(r\"\\d+\\. \", answer):\n",
    "#     answer = \"1. \" + answer.split(\"1. \")[1]\n",
    "# else:\n",
    "#     answer = \"1. \" + answer\n",
    "\n",
    "print(\n",
    "    f\"Writing questions to {question_list_filename}, with schema {schema}, with table name {table_name} and answer {answer}.\\n\\n\"\n",
    ")\n",
    "\n",
    "write_questions_to_file(question_list_filename, table_name, schema, answer)\n",
    "\n",
    "docs = load_json_file(question_list_filename)\n",
    "docs = create_docs_with_correct_metadata(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index         128\n",
       "table_name    448\n",
       "col_name      448\n",
       "type          448\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.memory_usage()ㅇㅇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
